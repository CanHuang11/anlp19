{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores identifying multiword expressions using the part-of-speech filtering technique of Justeson and Katz (1995), \"[Technical terminology: some linguistic properties and an algorithm for identification in text](https://brenocon.com/JustesonKatz1995.pdf)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('parser', <spacy.pipeline.DependencyParser at 0x1156d6bf8>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en', disable=['ner,parser'])\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser')\n",
    "\n",
    "# workaround for those getting an error loading the sapcy 'en' module:\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['ner,parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(filename):\n",
    "    \n",
    "    \"\"\" Read the first 1000 lines of an input file \"\"\"\n",
    "    tokens=[]\n",
    "    with open(filename) as file:\n",
    "        for idx,line in enumerate(file):\n",
    "            tokens.extend(nlp(line))\n",
    "            if idx > 1000:\n",
    "                break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465226\n"
     ]
    }
   ],
   "source": [
    "tokens=getTokens(\"../data/wiki.10K.txt\")\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[x.text for x in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify the POS tags to make the regex easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives=set([\"JJ\", \"JJR\", \"JJS\"])\n",
    "nouns=set([\"NN\", \"NNS\", \"NNP\", \"NNPS\"])\n",
    "\n",
    "taglist=[]\n",
    "for x in tokens:\n",
    "    if x.tag_ in adjectives:\n",
    "        taglist.append(\"ADJ\")\n",
    "    elif x.tag_ in nouns:\n",
    "        taglist.append(\"NOUN\")\n",
    "    elif x.tag == \"IN\":\n",
    "        taglist.append(\"PREP\")\n",
    "    else:\n",
    "        taglist.append(\"O\")\n",
    "                \n",
    "tags=' '.join(taglist)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChar2TokenMap(tags):\n",
    "    \n",
    "    \"\"\"  We'll search over the postag sequence, so we need to get the token ID for any\n",
    "    character to be able to match the word token. \"\"\"\n",
    "    \n",
    "    ws=re.compile(\" \")\n",
    "    char2token={}\n",
    "\n",
    "    lastStart=0\n",
    "    for idx, m in enumerate(ws.finditer(tags)):\n",
    "        char2token[lastStart]=idx\n",
    "        lastStart=m.start()+1\n",
    "        \n",
    "    return char2token\n",
    "\n",
    "def getToken(tokenId, char2token):\n",
    "    \n",
    "    \"\"\" Find the token ID for given character in the POS sequence \"\"\"\n",
    "    while(tokenId > 0):\n",
    "        if tokenId in char2token:\n",
    "            return char2token[tokenId]\n",
    "        tokenId-=1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2token=getChar2TokenMap(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find all sequences of POS tags that match the Justeson and Katz pattern of `(((ADJ|NOUN) )+|((ADJ|NOUN) )*(NOUN PREP )((ADJ|NOUN) )*)NOUN`\n",
    "\n",
    "\"In words, a candidate term is a multi-word noun phrase; and it either is a string of nouns and/or adjectives, ending in a noun, or it consists of two such strings, separated by a single preposition.\" (JK 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States 153\n",
      "Justice Sung 124\n",
      "Siu Chui 65\n",
      "police corruption 65\n",
      "New York 64\n",
      "Lai Sam 55\n",
      "Silver Oak 48\n",
      "first time 46\n",
      "Tit Tau 44\n",
      "World War II 41\n",
      "Puppet Master 41\n",
      "Early life 37\n",
      "United Kingdom 34\n",
      "Cho Kau 32\n",
      "Si Fu 31\n",
      "Los Angeles 30\n",
      "Kung Chan Yeung 29\n",
      "New Zealand 28\n",
      "police officers 27\n",
      "same year 25\n",
      "John Redcorn 25\n",
      "New York City 23\n",
      "New York Times 23\n",
      "general election 23\n",
      "Cairn India 23\n",
      "Florida College 23\n",
      "North America 22\n",
      "police force 22\n",
      "Personal life 22\n",
      "European Union 21\n",
      "same time 20\n",
      "Ling Lung 20\n",
      "Summer Olympics 20\n",
      "Broad Institute 19\n",
      "South Korea 18\n",
      "median income 18\n",
      "Donald Duck 18\n",
      "Barnett Shale 18\n",
      "Iskandar Muda 18\n",
      "following year 17\n",
      "Western Australia 17\n",
      "San Francisco 16\n",
      "Hong Kong 15\n",
      "Soviet Union 15\n",
      "Transparency International 15\n",
      "music video 15\n",
      "gold medal 15\n",
      "Sri Lanka 15\n",
      "Fort Worth Basin 15\n",
      "Brady Bunch 15\n",
      "British Columbia 14\n",
      "sea level 14\n",
      "same name 14\n",
      "next day 14\n",
      "National Register 14\n",
      "Police corruption 14\n",
      "Full Moon 14\n",
      "I. haynei 14\n",
      "South Africa 13\n",
      "Second World War 13\n",
      "20th century 13\n",
      "Gimmie Dat 13\n",
      "Military Road 13\n",
      "square mile 13\n",
      "19th century 13\n",
      "Abbie Hoffman 13\n",
      "Euphrates Region 13\n",
      "United Nations 12\n",
      "several years 12\n",
      "Historic Places 12\n",
      "Hamming distance 12\n",
      "Mauna Kea 12\n",
      "New Orleans EMS 12\n",
      "Mount Gilboa 12\n",
      "large number 11\n",
      "many years 11\n",
      "Track listing 11\n",
      "human rights 11\n",
      "Prime Minister 11\n",
      "housing units 11\n",
      "Ñāṇavīra Thera 11\n",
      "New South Wales 11\n",
      "Jerry Rubin 11\n",
      "Wadsley Bridge 11\n",
      "Puppet Master II 11\n",
      "Storm Hawks 11\n",
      "New Jersey 10\n",
      "other races 10\n",
      "Sai Kit 10\n",
      "late 1990s 10\n",
      "European Commission 10\n",
      "World War I 10\n",
      "racial makeup 10\n",
      "Supreme Court 10\n",
      "Villa College 10\n",
      "North Carolina 10\n",
      "Rabbi Weinblatt 10\n",
      "Muskoka Wharf 10\n",
      "Asuka period 10\n",
      "Pope Francis 10\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(\"(((ADJ|NOUN) )+|((ADJ|NOUN) )*(NOUN PREP )((ADJ|NOUN) )*)NOUN\")\n",
    "\n",
    "mweCount=Counter()\n",
    "\n",
    "for m in p.finditer(tags):\n",
    "    startToken=getToken(m.start(),char2token)\n",
    "    endToken=getToken(m.end(),char2token)\n",
    "    mwe=' '.join(words[startToken:endToken+1])\n",
    "    mweCount[mwe]+=1\n",
    "\n",
    "for k,v in mweCount.most_common(100):\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our MWE dictionary to be the 1000 most frequent sequences matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mwe=[k for (k,v) in mweCount.most_common(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform each MWE into a single token (e.g., replace `New York City` with `New_York_City`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMWE(text, mweList):\n",
    "    \n",
    "    \"\"\" Replace all instances of MWEs in text with single token \n",
    "    \n",
    "    MWEs are ranked from longest to shortest so that longest replacements are made first (e.g.,\n",
    "    \"New York City\" is matched first before \"New York\")\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_by_length = sorted(mweList, key=len, reverse=True)\n",
    "    for mwe in sorted_by_length:\n",
    "        text=re.sub(re.escape(mwe), re.sub(\" \", \"_\", mwe), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedText=replaceMWE(\"The New York Times is located in New York City\", my_mwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The New_York_Times is located in New_York_City\n"
     ]
    }
   ],
   "source": [
    "print(processedText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Plug in your own data into `getTokens` above and identify the MWE it contains.  How do you think MWE would perform for your classification task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
