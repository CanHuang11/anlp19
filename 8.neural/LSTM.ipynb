{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores LSTMs for text classification, representing a document by:\n",
    "\n",
    "* the final state of a LSTM\n",
    "* the final states of a Bidirectional LSTM\n",
    "* averaging the outputs of each time step in a BiLSTM\n",
    "* maxing the outputs of each time step in a BiLSTM\n",
    "\n",
    "This notebook also focuses on appropriate masking for averaging/max-pooling in padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Dense, Input, Embedding, GlobalAveragePooling1D, Lambda, Layer, Multiply, GlobalMaxPooling1D, Conv1D, Concatenate, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, max_vocab_size):\n",
    "\n",
    "    vocab={}\n",
    "    embeddings=[]\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        cols=file.readline().split(\" \")\n",
    "        num_words=int(cols[0])\n",
    "        size=int(cols[1])\n",
    "        embeddings.append(np.zeros(size))  # 0 = 0 padding if needed\n",
    "        embeddings.append(np.zeros(size))  # 1 = UNK\n",
    "        vocab[\"_0_\"]=0\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            val=np.array(cols[1:])\n",
    "            word=cols[0]\n",
    "            \n",
    "            embeddings.append(val)\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    return np.array(embeddings), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, vocab):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            # assumes text is already tokenized\n",
    "            text=cols[1].split(\" \")\n",
    "            X.append(cols[1])\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(docs, vocab, max_length=200):\n",
    "    \n",
    "    doc_ids=[]\n",
    "    \n",
    "    for doc in docs:\n",
    "        wids=[]\n",
    "\n",
    "        for token in doc[:max_length]:\n",
    "            val = vocab[token.lower()] if token.lower() in vocab else 1\n",
    "            wids.append(val)\n",
    "        \n",
    "        # pad each document to constant width\n",
    "        for i in range(len(wids),max_length):\n",
    "            wids.append(0)\n",
    "\n",
    "        doc_ids.append(wids)\n",
    "\n",
    "    return np.array(doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, vocab=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/text_classification_sample_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainText, trainY=read_data(\"%s/train.tsv\" % directory, vocab)\n",
    "devText, devY=read_data(\"%s/dev.tsv\" % directory, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = get_word_ids(trainText, vocab, max_length=200)\n",
    "devX = get_word_ids(devText, vocab, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "Y_train=np.array(le.transform(trainY))\n",
    "Y_dev=np.array(le.transform(devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    print (model.summary())\n",
    "    model.fit(trainX, Y_train, \n",
    "                validation_data=(devX, Y_dev),\n",
    "                epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll train a simple LSTM and represent the document by the summary vector output by the final state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_lstm(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "    \n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings],\n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    lstm = LSTM(lstm_size, return_sequences=False, activation='tanh', dropout=dropout_rate)(embedded_sequences)\n",
    "  \n",
    "    predictions=Dense(1, activation=\"sigmoid\")(lstm)\n",
    "\n",
    "    model = Model(inputs=word_sequence_input, outputs=predictions)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mashabelyi/anaconda3/envs/anlp/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mashabelyi/anaconda3/envs/anlp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 300)         15000600  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 25)                32600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 15,033,226\n",
      "Trainable params: 32,626\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/mashabelyi/anaconda3/envs/anlp/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 48 samples, validate on 6 samples\n",
      "Epoch 1/30\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.6971 - acc: 0.5417 - val_loss: 0.6877 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6877 - acc: 0.5000 - val_loss: 0.6884 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6963 - acc: 0.5417 - val_loss: 0.6873 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.7062 - acc: 0.4583 - val_loss: 0.6796 - val_acc: 0.6667\n",
      "Epoch 5/30\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.6698 - acc: 0.6875 - val_loss: 0.6740 - val_acc: 0.6667\n",
      "Epoch 6/30\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.6536 - acc: 0.7500 - val_loss: 0.6671 - val_acc: 0.8333\n",
      "Epoch 7/30\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.6562 - acc: 0.6250 - val_loss: 0.6567 - val_acc: 0.6667\n",
      "Epoch 8/30\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6493 - acc: 0.7083 - val_loss: 0.6417 - val_acc: 0.8333\n",
      "Epoch 9/30\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6184 - acc: 0.7917 - val_loss: 0.6205 - val_acc: 0.8333\n",
      "Epoch 10/30\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.5787 - acc: 0.9583 - val_loss: 0.5878 - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.5320 - acc: 0.9583 - val_loss: 0.5377 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.5179 - acc: 0.8750 - val_loss: 0.4786 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.4287 - acc: 0.9583 - val_loss: 0.4153 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3463 - acc: 1.0000 - val_loss: 0.3404 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2498 - acc: 1.0000 - val_loss: 0.2619 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2091 - acc: 1.0000 - val_loss: 0.2016 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1727 - acc: 0.9792 - val_loss: 0.1632 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.1065 - acc: 1.0000 - val_loss: 0.1350 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0711 - acc: 1.0000 - val_loss: 0.1024 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0468 - acc: 1.0000 - val_loss: 0.0818 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0316 - acc: 1.0000 - val_loss: 0.0704 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0408 - acc: 1.0000 - val_loss: 0.0614 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0275 - acc: 1.0000 - val_loss: 0.0554 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.0541 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.0503 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.0445 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.0411 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.0387 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0368 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.0353 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "train(get_simple_lstm(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll represent a document by two concatenated vectors: the output of the final state of a forward LSTM and the output of the final state of the backward LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_bilstm(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings],\n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    bi_lstm = Bidirectional(LSTM(lstm_size, return_sequences=False, activation='tanh', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "  \n",
    "    predictions=Dense(1, activation=\"sigmoid\")(bi_lstm)\n",
    "\n",
    "    model = Model(inputs=word_sequence_input, outputs=predictions)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 300)         15000600  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50)                65200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 15,065,851\n",
      "Trainable params: 65,251\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 48 samples, validate on 6 samples\n",
      "Epoch 1/30\n",
      "48/48 [==============================] - 3s 70ms/step - loss: 0.7267 - acc: 0.5000 - val_loss: 0.6928 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.7220 - acc: 0.4375 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6993 - acc: 0.4792 - val_loss: 0.6902 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.7015 - acc: 0.5000 - val_loss: 0.6838 - val_acc: 0.5000\n",
      "Epoch 5/30\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6887 - acc: 0.4583 - val_loss: 0.6761 - val_acc: 0.8333\n",
      "Epoch 6/30\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6837 - acc: 0.5625 - val_loss: 0.6672 - val_acc: 0.6667\n",
      "Epoch 7/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.6704 - acc: 0.7083 - val_loss: 0.6557 - val_acc: 0.8333\n",
      "Epoch 8/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.6536 - acc: 0.7292 - val_loss: 0.6419 - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.6672 - acc: 0.6667 - val_loss: 0.6265 - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.6583 - acc: 0.8125 - val_loss: 0.6111 - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.6162 - acc: 0.8125 - val_loss: 0.5908 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.6067 - acc: 0.8542 - val_loss: 0.5605 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.5664 - acc: 0.9375 - val_loss: 0.5247 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.5258 - acc: 0.9583 - val_loss: 0.4867 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.4691 - acc: 0.9792 - val_loss: 0.4492 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.4284 - acc: 1.0000 - val_loss: 0.4122 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.3704 - acc: 1.0000 - val_loss: 0.3723 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3211 - acc: 1.0000 - val_loss: 0.3310 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2652 - acc: 1.0000 - val_loss: 0.2948 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2338 - acc: 1.0000 - val_loss: 0.2646 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.1960 - acc: 1.0000 - val_loss: 0.2375 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.1671 - acc: 1.0000 - val_loss: 0.2120 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.1502 - acc: 1.0000 - val_loss: 0.1877 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.1222 - acc: 1.0000 - val_loss: 0.1629 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.1031 - acc: 1.0000 - val_loss: 0.1399 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.0883 - acc: 1.0000 - val_loss: 0.1194 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.0732 - acc: 1.0000 - val_loss: 0.1022 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.0611 - acc: 1.0000 - val_loss: 0.0871 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.0468 - acc: 1.0000 - val_loss: 0.0721 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.0408 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "train(get_simple_bilstm(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final state is often a poor representation of the entire sequence, since it can lose information from the beginning of the sequence.  Let's define a few other layers that can aggregate information across the *entire* sequence, using the information that's output from the LSTM at each time step.  We need to define these custom layers in keras to accomodate zero-padding appropriately (see [here](https://stackoverflow.com/questions/39510809/mean-or-max-pooling-with-masking-support-in-keras/39534110#39534110) for discussion, where these functions originate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAveragePooling1D(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(MaskedAveragePooling1D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.repeat(mask, x.shape[-1])\n",
    "            mask = tf.transpose(mask, [0,2,1])\n",
    "            # zero out the elements of x that are masked\n",
    "            x = x * mask\n",
    "            \n",
    "        # sum the modified input, but normalize only over the number of non-masked time steps\n",
    "        return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMaxPooling1D(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(MaskedMaxPooling1D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            # take the logical negation of the mask (all 1s become 0 and 0s become 1s)\n",
    "            mask=tf.logical_not(mask)\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.repeat(mask, x.shape[-1])    \n",
    "            mask = tf.transpose(mask, [0,2,1])\n",
    "            \n",
    "            # subtract a big number from each masked input (so that it won't be the max)\n",
    "            mask *= 10000\n",
    "            x = x - mask\n",
    "        \n",
    "        # max over the modified input along the temporal dimension\n",
    "        return K.max(x, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore the use of these functions to aggregate information over the whole sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_with_average_pooling(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings], \n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    x = Bidirectional(LSTM(lstm_size, return_sequences=True, activation='tanh', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "    x=MaskedAveragePooling1D()(x)\n",
    "\n",
    "    x=Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=word_sequence_input, outputs=x)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, None, 300)         15000600  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 50)          65200     \n",
      "_________________________________________________________________\n",
      "masked_average_pooling1d_1 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 15,065,851\n",
      "Trainable params: 65,251\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 48 samples, validate on 6 samples\n",
      "Epoch 1/30\n",
      "48/48 [==============================] - 3s 54ms/step - loss: 0.7212 - acc: 0.3750 - val_loss: 0.6903 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.6867 - acc: 0.4792 - val_loss: 0.6709 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.6629 - acc: 0.6875 - val_loss: 0.6534 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.6630 - acc: 0.6250 - val_loss: 0.6336 - val_acc: 0.5000\n",
      "Epoch 5/30\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.6509 - acc: 0.7083 - val_loss: 0.6139 - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.6255 - acc: 0.9583 - val_loss: 0.5949 - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.6103 - acc: 0.9167 - val_loss: 0.5744 - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.5916 - acc: 0.9375 - val_loss: 0.5519 - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.5681 - acc: 0.9583 - val_loss: 0.5269 - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.5434 - acc: 0.9583 - val_loss: 0.5006 - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.5145 - acc: 1.0000 - val_loss: 0.4751 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.4968 - acc: 1.0000 - val_loss: 0.4432 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.4701 - acc: 1.0000 - val_loss: 0.3967 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.4421 - acc: 1.0000 - val_loss: 0.3473 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3870 - acc: 0.9792 - val_loss: 0.3037 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3526 - acc: 1.0000 - val_loss: 0.2624 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3063 - acc: 1.0000 - val_loss: 0.2222 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.2567 - acc: 1.0000 - val_loss: 0.1862 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.2105 - acc: 1.0000 - val_loss: 0.1559 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.1677 - acc: 1.0000 - val_loss: 0.1305 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.1489 - acc: 1.0000 - val_loss: 0.1106 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1198 - acc: 1.0000 - val_loss: 0.0982 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.0987 - acc: 1.0000 - val_loss: 0.0870 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0846 - acc: 1.0000 - val_loss: 0.0743 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.0763 - acc: 1.0000 - val_loss: 0.0642 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0625 - acc: 1.0000 - val_loss: 0.0563 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.0673 - acc: 1.0000 - val_loss: 0.0535 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.0473 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.0460 - acc: 1.0000 - val_loss: 0.0540 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.0423 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "train(get_bilstm_with_average_pooling(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_with_max_pooling(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings], \n",
    "                                     mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    x = Bidirectional(LSTM(lstm_size, return_sequences=True, activation='tanh', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "    x=MaskedMaxPooling1D()(x)\n",
    "\n",
    "    x=Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=word_sequence_input, outputs=x)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, None, 300)         15000600  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, None, 50)          65200     \n",
      "_________________________________________________________________\n",
      "masked_max_pooling1d_1 (Mask (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 15,065,851\n",
      "Trainable params: 65,251\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/mashabelyi/anaconda3/envs/anlp/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 48 samples, validate on 6 samples\n",
      "Epoch 1/30\n",
      "48/48 [==============================] - 3s 63ms/step - loss: 0.7062 - acc: 0.5000 - val_loss: 0.6663 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.6593 - acc: 0.6667 - val_loss: 0.6357 - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.6466 - acc: 0.7292 - val_loss: 0.6041 - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.6272 - acc: 0.7292 - val_loss: 0.5700 - val_acc: 1.0000\n",
      "Epoch 5/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.5680 - acc: 0.9375 - val_loss: 0.5309 - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.5291 - acc: 1.0000 - val_loss: 0.4944 - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.5048 - acc: 1.0000 - val_loss: 0.4611 - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.4782 - acc: 1.0000 - val_loss: 0.4291 - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.4377 - acc: 1.0000 - val_loss: 0.3938 - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.4114 - acc: 1.0000 - val_loss: 0.3569 - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3710 - acc: 1.0000 - val_loss: 0.3272 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3174 - acc: 1.0000 - val_loss: 0.2982 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2901 - acc: 1.0000 - val_loss: 0.2720 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2580 - acc: 1.0000 - val_loss: 0.2471 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2311 - acc: 1.0000 - val_loss: 0.2243 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2149 - acc: 1.0000 - val_loss: 0.2015 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.1834 - acc: 1.0000 - val_loss: 0.1803 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.1563 - acc: 1.0000 - val_loss: 0.1621 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.1454 - acc: 1.0000 - val_loss: 0.1458 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.1249 - acc: 1.0000 - val_loss: 0.1321 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.1113 - acc: 1.0000 - val_loss: 0.1213 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.0978 - acc: 1.0000 - val_loss: 0.1122 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.0897 - acc: 1.0000 - val_loss: 0.1036 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.0777 - acc: 1.0000 - val_loss: 0.0953 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.0713 - acc: 1.0000 - val_loss: 0.0881 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.0649 - acc: 1.0000 - val_loss: 0.0822 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.0570 - acc: 1.0000 - val_loss: 0.0770 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.0510 - acc: 1.0000 - val_loss: 0.0721 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0475 - acc: 1.0000 - val_loss: 0.0678 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.0450 - acc: 1.0000 - val_loss: 0.0638 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "train(get_bilstm_with_max_pooling(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
